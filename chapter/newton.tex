\chapter{Newton's Method}\label{chp:Newton Method}

\section{Motivation}




In this chapter we study the choice of search directions used in our basic updating scheme
\begin{align*}
    x_{k+1} = x_k +\alpha_k d_k. 
\end{align*}
for solving
\begin{align*}
    \mathcal{P}:\min_{x\in\R^n} f(x).
\end{align*}
In the previous lecture, we saw the general framework of descent
algorithms, with several choices for the step size and the descent
direction. Today, we see a wonderful descent method with superlinear (in fact
quadratic) rate of convergence: the Newton algorithm.
\par 
The Newton's method is nothing but a descent method with a specific
choice of a descent direction; one that iteratively adjusts itself to the local
geometry of the function to be minimized.
\par
In practice, Newton's method can converge with much fewer iterations
than gradient methods. For example, for quadratic functions, while we
saw that gradient methods can zigzag for a long time (depending on the
underlying condition number), Newton's method will always get the
optimal solution in a single step. The cost that we pay for fast convergence is the need to 
\par
(1) access second order information (i.e., derivatives of first and second order), and 
\par
(2) solve a linear system of equations at every step of the algorithm.

\section{Pure Newton method }
Recall the general form of our descent methods:
\begin{align*}
    x_{k+1}=x_k+\alpha_kd_k.
\end{align*}
Let us have $\alpha_k=1,\forall k$ for now.  So we take full steps at
each iteration. (This is sometimes called the pure
Newton method.) Newton's method is simply the following choice for the
descent direction:
\begin{align*}
    d_k = -(\nabla^2 f(x))^{-1}\nabla f(x_k).
\end{align*}
And iteration only well-defined when $\nabla^2 f(x)$ is invertible.
\par 
But Where does $d_k$ come from? One motivation is minimizing a quadratic approximation of the function
sequentially. 
\par
Around the current iterate $x_k$, $f(x)$ can be approximated by:
\begin{align*}
    f(x)\approx q(x):= f(x_k)+\nabla f(x_k)^T(x-x_k)+\frac{1}{2}(x-x_k)^T\nabla^2f(x_k)(x-x_k),
\end{align*}
which is the quadratic Taylor expansion of $f(x)$ at $x = x_k$. Notice that $q(x)$ is a quadratic function, which is minimized by solving
$\nabla q(x) = 0$. Since the gradient of $q(x)$ is:
\begin{align*}
    \nabla q(x) = \nabla f(x_k) + \nabla^2 f(x_k)(x-x_k), 
\end{align*}
we therefore are motivated to solve:
\begin{align*}
    \nabla f(x_k) + \nabla^2 f(x_k)(x-x_k) = 0,
\end{align*}
which yields
\begin{align*}
    x-x_k = -(\nabla^2 f(x))^{-1}\nabla f(x_k). 
\end{align*}
This leads to the pure Newton method for solving $\mathcal{P}$.


\textcolor{red}{add: solve least square using pure newton method}

\section{Quadratic Convergence of Newton's Method}

\begin{proposition}{}{}
    If $\nabla^2 f(x)\succ 0$ and $d=-(\nabla^2 f(x))^{-1}f(x)\neq 0$, then $d$ is a desent direction. 
\end{proposition}
\begin{proof}
    It is sufficient to show that $\nabla f(x)^T d=-\nabla f(x)^T\nabla^2 f(x)^{-1}\nabla f(x)<0$. 
    This will clearly be the case if $(\nabla^2 f(x))^{-1}\succ 0$. Since $\nabla^2 f(x)\succ 0$, if $y\neq 0$,
    \begin{align*}
        0<(\nabla^2 f(x)^{-1} y)^T\nabla^2f(x)(\nabla^2 f(x)^{-1}y) = y^T\nabla^2 f(x)^{-1}y,
    \end{align*}
    thereby showing that $\nabla^2f(x)^{-1}\succ0$.
\end{proof}

\begin{proposition}{}{}
    Suppose that $f(x)$ is twice differentiable. Then
    \begin{align*}
        \nabla f(z) - \nabla f(x) = \int_{0}^{1} \nabla^2f(x+t(z-x))(z-x) dt.
    \end{align*}
\end{proposition}
\begin{proof}
    Let $\phi(t)=\nabla f(x+t(z-x))$. Then $\phi(0)=\nabla f(x)$ and $\phi(1) = \nabla f(z)$, 
    and $\phi'(t)= \nabla^2 f(x+t(z-x)) (z-x)$. From Newton-Leibniz theorem, we have 
    \begin{align*}
        \nabla f(z) - \nabla f(x) &= \phi(1)-\phi(0)\\
                                  &= \int_{0}^{1} \phi'(t) dt\\
                                  &= \int_{0}^{1}\nabla^2 f(x+t(z-x))(z-x)dt.
    \end{align*}
\end{proof}


\begin{theorem}{}{}
    Suppose $f(x)$ is twice continuously differentiable and $x^*$ is a point for which $\nabla f(x^*)=0,\nabla^2 f(x^*)\succ 0$.
    Suppose that $\nabla^2 f(x)$ is Lipschitz continuous at a neighbourhood of $x^*$ (denoted by $\mathcal{N}_{\delta}(x^*)$), i.e.
    \begin{align*}
        ||\nabla^2 f(x)-\nabla^2 f(y)||\leq L||x-y||, \forall x,y \in \mathcal{N}_{\delta}(x^*).
    \end{align*}
    Then:\\
    (1) iterative sequence $\{x_k\}$ converges to $x^*$ if inital point is close to $x^*$(local convergence).\\
    (2) the convergence rate of (i) is  quadratic.\\
    (3) sequence $\{||\nabla f(x_k)||\}$ converges to $0$ and the convergence rate is quadratic.
\end{theorem}
\begin{proofsolution}
    Since $\nabla f(x^*)=0$, we have\\
    \textcolor{red}{$\nabla^2 f(x_k)^{-1}$ is pd since $x_k$ close to $x^*$ }
    \begin{align*}
        x_{k+1} - x^* &= x_k - \nabla^2 f(x_k)^{-1}\nabla f(x_k)-x^*\\
                      &= (x_k - x^*)-\nabla^2 f(x_k)^{-1}\nabla f(x_k)
                      &= \nabla^2 f(x_k)^{-1}(\nabla^2 f(x)(x_k-x^*)-(\nabla f(x_k)-\nabla f(x^*))).
    \end{align*}
\end{proofsolution}

In practice, to guarante the local convergence, we usually firstly use gradient descent to  

\section{Modified Newton Method}

There are some drawbacks of pure Newton method:
\par
(1) When $\nabla^2 f(x)$ is not positve-definite, Newton's direction may not be a descent direction.
\par
(2) When the inital point is far away from the optimal point, by choosing $\alpha_k=1$, we may not be able to obtain a convergent iterative sequence. 
This is because the Taylor approximation is limited in region.

\par 
Hence, something should be done to modified pure Newton's Method. 
In this section, we firstly introduce the modified Newton method with line search. 
The basic idea is: 
\par
(1) modified $\nabla^2 f(x)$ so that we can get a positve definite matrix. 
\par
(2) use line search method to choose step size.

\begin{algorithm}[H]
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{current solution $x_k$}
    \Output{the next solution $x_{k+1}$}
    \BlankLine
    determined matrix $E_k$ such that $B_k\overset{def}{=} \nabla^2 f(x_k) + E_k$ is postive-definite and the condition number is small. \\
    solve the modified newton equation $B_kd_k=-\nabla f(x_k)$ to obtain desent direction $d_k$.\\
    use any line search condition to choose stepsize $\alpha_k$.\\
    \Return $x_{k+1}=x_k+\alpha_k d_k$
    \caption{modified Newton method with line search at iteration $k$}
    \label{alg:mnmls}
\end{algorithm}

\textcolor{red}{when the dimension is too big, the calculation of modified newton method is also expansive}

\section{Reference}
\begin{itemize}
    \item \href{https://www.princeton.edu/~aaa/Public/Teaching/ORF363_COS323/F23/ORF363_COS323_F23_Lec9.pdf}{lecture notes from princeton university}
    \item \href{https://ocw.mit.edu/courses/15-084j-nonlinear-programming-spring-2004/83159d56de04a7e7dc94d0348fa4ccda_lec3_newton_mthd.pdf}{lecture notes from MIT}
\end{itemize}